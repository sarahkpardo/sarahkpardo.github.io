---
title: 'On antiderivatives of smoothed step functions'
date: 2023-11-10
permalink: /posts/2023/11/dirac-step-and-ramp/
tags:
  - exposition
  - ramp
  - activation functions
---

DRAFT

On antiderivatives of smoothed step functions
======

Consider the ****ramp**** function: $R(x) = \max\\{0, x\\}.$ Deep learning people usually think they discover everything for the first time, and they have called it the ReLU function, but ramp has been around doing conceptual work elsewhere considerably before 2007. 

Here are a few facts about ****ramp****:

- $R(x)$ is the antiderivative of the Heaviside step function, which is defined as

$$
H(x) := \begin{cases}
1, \quad &x > 0\\
\frac{1}{2}, \quad &x = 0\\
0, \quad &x < 0
\end{cases},
$$

so that $R(x) = \int_{-\infty}^{x}H(\xi)d\xi = xH(x) = \max\\{0, x\\}.$
- In turn, the distributional derivative of $H(x)$ is the Dirac delta function: $\frac{dH(x)}{dx} = \delta(x).$
- $R(x) = x\delta(x > 0).$
- $R(x)$ is the mean of an independent variable and its absolute value: $\frac{x + |x|}{2}$.
- $R(x)$ satisfies the differential equation
  $$\frac {d^{2}}{dx^{2}}R(x-x_{0}) = \delta (x-x_{0}),$$
  which means that $R(x)$ is a Green's function for the second derivative operator.
   
The family of functions known as “sigmoid” curves are legible as different choices of smooth approximation to $H(x):$

$\dots$
