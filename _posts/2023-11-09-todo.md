---
title: 'Things on the todo list'
date: 2023-11-09
permalink: /posts/2023/11/todo/
tags:
  - todo
  - orthogonal matching pursuit
  - attention
  - activation functions
  - leverage scores
---

### Attention mechanisms
Write out all my favorite equivalences in one place, including
- [kernels](https://aclanthology.org/D19-1443)
- [Galerkin methods](https://arxiv.org/abs/2105.14995)
- [boundary value inverse problems](http://arxiv.org/abs/2209.14977)
- [block Toeplitz matrices and differential equations on graphs](https://proceedings.mlr.press/v162/choromanski22a.html)
- [recurrence relations](https://proceedings.mlr.press/v119/katharopoulos20a.html)
- [tensor decomposition](http://arxiv.org/abs/1905.01289) and various algorithms for it
- and more

### Activation functions
- contemplating their overlap with [gating](http://arxiv.org/abs/1804.04849) mechanisms, doing roughly the same thing but applying and combining learned rotations to the input
- in general looking for every place where we have a product of a thing and the output of a function on the thing (generically $xf(x)$ ) vs a sum ( $x + f(x)$ ); "or" vs "and"
- that [GELU](https://openreview.net/forum?id=Bk0MRI5lg) combinines ReLU and multiplication with a Bernoulli-distributed random variable
- rectifier units in analog to digital converters
- on ramp and other antiderivatives of cumulative distribution functions

### Leverage scores
- [Fourier Sparse Leverage Scores and Approximate Kernel Learning](http://arxiv.org/abs/2006.07340)

### LayerNorm
- as [operator normalization](https://arxiv.org/abs/2105.14995)
<!--- ; that we're not concerned about expanding/contracting space so much as learning appropriate rotations, and inducing properties on the distributions via activation functions and whatever other stuff we do outside the linear transforms --->
- operator norm regularization also [implicated in adversarial training](https://proceedings.neurips.cc/paper/2020/hash/ab7314887865c4265e896c6e209d1cd6-Abstract.html)

### Distributions
The distributions of weights and activations as the salient empirical properties in a model:
- [Guth et al.](http://arxiv.org/abs/2305.18512): "that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit...the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features." $\rightarrow$ porting this from CNNs to LLMs?
- [Waves in Neural Media](http://link.springer.com/10.1007/978-1-4614-8866-8): if we care to link this back to the metaphor which started it all
- [sparse recovery from random measurements via orthogonal matching pursuit](http://ieeexplore.ieee.org/document/4385788/): noting well the required properties of the the random measurement matrix
- this all re: [products of experts](https://doi.org/10.1162/089976602760128018)
- ...in the end, [it's all random matrix theory isn't it](https://doi.org/10.1088/0305-4470/33/26/102)

### Complex-valued data, models, and devices
- data: e.g. raw MRI k-space measurements
- models: e.g. complex-valued neural networks
- devices: e.g. in situ computation on optical chips
